import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_excel("DATA-FINAL.xls",usecols=[3,4,5,6,7,8,16,18,9])
print("Head:", df.head())
print("Tail:", df.tail())
print(df)
#df with Integrated Bachelor of Technology - Master of Technology (Biotechnology)
df=df[df['MHRDName'] == "Integrated Bachelor of Technology - Master of Technology (Biotechnology"]
print(df)
#mapping grade into numerical value
grade_map={'O':1,'A+':2,'A':3,'B+':4,'B':5,'C':6,'D':7,'PASS':8,'E':9,'F':10,'FAIL':10,'R':11,'ReApp':11,'I':12,'M':13,'S':14}
#print(grade_map)
df['Grade']=df['Grade'].map(grade_map)
# ScholarType: Hostelar vs. Day-Scholar

plt.figure(figsize=(6,6))
sns.countplot(df.ScholarType, palette='tab20b_r')
plt.title('Hostelar and Day-Scholar Students Count', fontsize=20)
plt.xlabel('Living Area', fontsize=16)
plt.ylabel('Number Of Students', fontsize=16)
plt.show()
#mapping of gender and scholar type
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['ScholarType']=le.fit_transform(df['ScholarType'].values)
df['Gender']=le.fit_transform(df['Gender'].values)
print(df['ScholarType'].unique())
print(df['Gender'].unique())
# look for missing values
print(df.isnull().any())
#imputer to preprocess data
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer
imr=SimpleImputer(missing_values=np.nan,strategy='most_frequent')
imr=imr.fit(df.iloc[:,1:6])
df.iloc[:,1:6]=imr.transform(df.iloc[:,1:6].values)
imr=imr.fit(df.iloc[:,7:9])
df.iloc[:,7:9]=imr.transform(df.iloc[:,7:9].values)
print(df)
print(df.isnull().any())
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
sns.set_style('whitegrid')
import statsmodels.api as sm
# converting difficulty level of course to categorical variable into easy, med and hard
df['difficulty'] = 'na'
df.loc[(df.Grade >= str(1)) & (df.Grade <= str(4)), 'difficulty'] = 'easy' 
df.loc[(df.Grade >= str(5))& (df.Grade <= str(9)), 'difficulty'] = 'med' 
df.loc[(df.Grade >= str(10)) & (df.Grade <= str(14)), 'difficulty'] = 'hard'
# difficulty level Countplot
plt.figure(figsize=(8,6))
sns.countplot(df.difficulty, order=["easy","med","hard"], palette='Set1')
plt.title('Difficulty level based on grades',fontsize=20)
plt.xlabel('Difficulty lvel', fontsize=16)
plt.ylabel('Number of Student', fontsize=16)
#mapping difficulty level into numerical value


difficulty_map={'easy':1,'med':2,'hard':3}
df['difficulty']=df['difficulty'].map(difficulty_map)
print(df)
new_df = df.copy()
new_df = new_df.drop(['MHRDName'], axis=1)
print(new_df)
# see correlation between variables through a correlation heatmap
corr = df.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corr, annot=True, cmap="Blues")
plt.title('Correlation Heatmap', fontsize=20)
# difficulty level Countplot
plt.figure(figsize=(8,6))
sns.countplot(df.difficulty, order=["easy","med","hard"], palette='Set1')
plt.title('Difficulty level based on grades',fontsize=20)
plt.xlabel('Difficulty lvel', fontsize=16)
plt.ylabel('Number of Student', fontsize=16)
# dataset train_test_split
from sklearn.model_selection import train_test_split
X = new_df.drop('difficulty',axis=1)
y = new_df.difficulty
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)
#classification 
#SVM
from sklearn.svm import SVC
svc = SVC()
s= svc.fit(X_train, y_train)
print("SVC Model Score" , ":" , s.score(X_train, y_train) , "," ,
      "Cross Validation Score" ,":" , s.score(X_test, y_test))
#ada boost classification
from sklearn.ensemble import AdaBoostClassifier
ada = AdaBoostClassifier(n_estimators=1)
af = ada.fit(X_train, y_train)
print("Ada Boost Model Score" , ":" , af.score(X_train, y_train) , "," ,
      "Cross Validation Score" ,":" , af.score(X_test, y_test))

#Stochastic Gradient Descent Classification
from sklearn.linear_model import SGDClassifier
sgd = SGDClassifier()
sf = sgd.fit(X_train, y_train)
print("Stochastic Gradient Descent Model Score" , ":" , sf.score(X_train, y_train) , "," ,
      "Cross Validation Score" ,":" , sf.score(X_test, y_test))
